\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{xcolor}

% Configuration des commentaires (TodoNotes)
% Les notes s'afficheront dans la marge en orange.
\usepackage[colorinlistoftodos, prependcaption, textsize=small]{todonotes}

% Commandes pour les hypothèses
\newcommand{\Hnull}{\mathcal{H}_0}
\newcommand{\Hone}{\mathcal{H}_1}

\title{Correction Draft: Statistical Analysis for Nulling Interferometry}
\author{Vincent}
\date{\today}

\begin{document}

\maketitle

\listoftodos[Liste des corrections demandées par le superviseur]
\newpage

\section{Introduction \& Distribution Fitting}

\todo[inline, color=orange!40]{Note générale (Haut de page 1) : Pour le lecteur : le but est d'en faire un expert en lui donnant toutes les infos pour qu'il le devienne en lisant le papier et puisse reproduire les résultats s'il le souhaite. C'est le propre de la démarche scientifique : éviter d'être juste "déclaratif" (arguments d'autorité), préférer systématiquement être aussi précis que possible (arguments d'expertise).}

Among the tested laws, the Cauchy distribution seems to offer a relatively satisfactory fit (Fig. 2), although not perfect, particularly on the tails when the number of samples is high.

\todo{Commentaire Fig 2 : "C'est faible et pas quantifié. Vu la figure 2, ça ne colle pas comme tu le dis."}

This observation guides the choice of statistical tests to favor, particularly those effective for detecting shifts in symmetric distributions.

\todo{Important : Justifier théoriquement et/ou empiriquement (via Poptique et les simus) si les distributions doivent être symétriques.}
\todo{Suggestion : Ajouter éventuellement des acquisitions de frames sur banc pour appuyer à un moment ces hypothèses.}

Furthermore, this suggests that data fitting by minimizing mean squared error (MSE) will very likely not be very efficient due to the heavy tails of the distribution.

\todo{Correction : "Pas le fit d'une distribution empirique par une autre en tout cas... on en discutera."}

Instead, we should use a cost function derived from the Cauchy law, which is more robust to outliers:
\begin{equation}
    Cost(x,y)=\sum_{i}\log(1+(y_{i}-s(x_{i}))^{2})
\end{equation}

\todo[color=red!40]{Critique : "Non, je pense que la fin de la section n'est pas bonne."}

\section{Implemented statistical tests}

In the presence of a companion (hypothesis $\Hone$), the Kernel-Nulling output distribution is modified compared to the star-alone case (hypothesis $\Hnull$). This modification manifests primarily as a distribution shift, although other changes in distribution shape may also occur. Depending on conditions, it is for example possible to observe a flattening of the distribution.

\subsection{Mean}
The most natural test to perform under these conditions consists of comparing the mean to a threshold. Given that the planet can induce a distribution shift in both directions, we use the absolute value of the mean.

In practice, we observe that the mean is here a particularly unreliable test statistic (figure 3). Indeed, the latter is sensitive to extreme values, yet the considered distributions have heavy tails.

\subsection{Median}
Another natural test statistic in this context is the median, which is more robust to extreme values than the mean. The median is defined as the central value of a sorted dataset.
As with the mean, we use the absolute value of the median to capture shifts in both directions which we then compare to a threshold.

Surprisingly, the median proves to be a very efficient test statistic in this context, surpassing many more sophisticated tests. However, it requires a large number of samples to be effective.

\subsection{Kolmogorov-Smirnov}
The Kolmogorov-Smirnov (KS) test is a non-parametric test that compares two empirical distributions. It is particularly useful when distributions do not follow a known probability law.
The KS test statistic is defined as:
\begin{equation}
    D_{KS} = \sup_{x} |F_1(x) - F_2(x)| \gtrless_{\Hnull}^{\Hone} \xi
\end{equation}
In practice, the KS test proves to be a robust and efficient test statistic in this context, although it also requires a large number of samples.

\subsection{Flattening}
After studying the distribution shift via median and mean, we investigated the flattening effect we could observe in certain configurations. For this, we defined a test statistic we call "flattening", which measures the mean of absolute deviations between each sample and the sample median.

\begin{equation}
    D_{f}=\frac{1}{N}\sum_{i=1}^{N}|x_{i}-\bar{x}| \gtrless_{\Hnull}^{\Hone} \xi
\end{equation}

\todo{Note manuscrite : "Il faut l'appliquer sur un bouton et regarder si ça marche." (Référence probable à une manip sur le banc ou une vérif rapide).}

The flattening test alone is not intended to be a very efficient test statistic, but it highlights the fact that flattening is a real effect that can be exploited.

\subsection{Median of Absolute Values}
This statistic measures the median of absolute values of samples, offering a robust measure of both a shift and flattening of the distribution.
This test statistic proves to be one of the most efficient with numerically obtained distributions.

\subsection{Other tests considered}
We also explored other statistical tests (Cramér-von Mises, Wilcoxon-Mann-Whitney, Anderson-Darling, Brunner-Munzel). However, these tests did not show satisfactory performance.

\section{Missing Approach: Generalized Likelihood Ratio (GLR)}

\todo[inline, color=red!40, size=\large]{AJOUT MAJEUR REQUIS (Page 3)}
\todo[inline]{"Il manque aussi une 3ème approche ici qui est un GLR utilisant le modèle direct pour calculer la vraisemblance des données selon la position du compagnon."}
\todo[inline]{"Le GLR cherche alors à maximiser la position du compagnon. C'est les formules que j'avais mises au tableau dans mon bureau et la photo est sur Discord."}
\todo[inline]{"On en rediscute aussi, je sais que ça t'avait fait gamberger."}

\section{Generalization (New Section Required)}

\todo[inline, color=blue!40]{NOUVELLE SECTION REQUISE : Généralisation}
\todo[inline]{"C'est important parce que ce test (GLR) permet la généralisation à plusieurs kernels."}
\todo[inline]{"Il faut ajouter une section : Généralisation des tests considérés à plusieurs kernels et/ou poses (= comment les stat de tests mono-kernel obs peuvent se combiner)."}

\section{Likelihood Ratio (LR)}

The likelihood ratio (LR) test consists of comparing the probability of observing the data under each hypothesis:
\begin{equation}
    \Lambda(x)=\frac{p(x;\Hone)}{p(x;\Hnull)}
\end{equation}
However, it requires analytical knowledge of distributions under each hypothesis.

\todo{Correction : "Non ! Le LR connait tout..." (Remarque sur le fait que le LR théorique a l'info complète, à clarifier).}

To compare the previous statistical tests to this optimal test, we propose an approach based on parametric model fitting.
Let us first consider the classical case where, under each hypothesis, the distribution is Gaussian.

\section{Conclusion / Bilan}

\todo[inline, color=green!40]{CONCLUSION À REFAIRE (Page 5)}
\todo[inline]{"À la fin en bilan, fais une liste des PROS et CONS de chaque méthode."}
\todo[inline]{"Il faut que tu déploies dans ce papier une analyse exhaustive et convaincante de la nature des données et des régimes où l'on peut se trouver, des types de distributions suivant les régimes, et des types de tests qu'on peut utiliser."}
\todo[inline]{"À la fin on se dira que tu as plié le problème, le spécialiste de la détection sur des données Kernel-Nulling c'est toi !"}

\end{document}