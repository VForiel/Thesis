# Multithread dataset generation in neural_calibration

## Summary
- Parallelized dataset generation in `src/analysis/neural_calibration.py` using `ThreadPoolExecutor`.
- Each sample now uses a deep-copied `Context` to avoid race conditions on shared mutable state (chip phases).
- Added optional `max_workers` parameter; defaults to all logical cores via `os.cpu_count()`. If `psutil` is present, logs physical core count.

## Files Modified
- `src/analysis/neural_calibration.py`

## Changes
- Function signature: `generate_dataset(context, n_samples=100, n_steps=20, max_workers=None)`.
- Internal: created `_generate_one_sample` helper run in threads; uses `copy.deepcopy(context)` per sample.
- Progress reporting kept with `tqdm` via `as_completed`.
- Cache behavior unchanged; filename remains based on `n_samples`, `n_steps`, and `Γ`.

## Tests
- Intended quick smoke test via a minimal context, but constructors differ from assumptions. Please validate by running your existing workflow:

```powershell
python src/analysis/neural_calibration.py --samples 200 --steps 20 --epochs 10 --lr 0.001 --dropout 0.0 --note "Threaded dataset gen" --no-plot
```

Optionally control workers:
```powershell
python src/analysis/neural_calibration.py --samples 200 --steps 20 --no-plot
```

## Physics Validation
- No change to physics: each sample generates random phase perturbations, computes `phase_map` via `get_phase_map`, and targets remain `(-σ) % 2π`.
- Deep copy ensures per-sample context integrity; `get_phase_map` behavior unchanged.

## Migration Notes
- If you prefer processes (for isolation), we can switch to `ProcessPoolExecutor` and construct a fresh `Context` per worker; needs a serializable factory for the instrument.
- If `psutil` is available, physical core count is logged but not used; we can add a flag to prefer physical cores if desired.
