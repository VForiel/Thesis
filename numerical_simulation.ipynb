{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# üìú **Abstract**\n",
    "\n",
    "</div>\n",
    "\n",
    "## **Breaking the $10^{-8}$ Contrast Barrier: Toward the First Adaptive Photonic Nulling Interferometers for Direct Exoplanet Detection**\n",
    "\n",
    "The quest for Earth-like exoplanets demands unprecedented instrumental capabilities: contrasts exceeding $10^{-8}$, milliarcsecond angular resolution, and stability over hours of observation. We present one of the first implementations of an **adaptive photonic nulling interferometer**, featuring 14 real-time electro-optic phase shifters integrated within a silicon nitride photonic chip, improving astronomical instrumentation through **on-chip wavefront control without mechanical components**.\n",
    "\n",
    "This breakthrough instrument combines **Kernel-Nulling interferometry** with **adaptive photonics**, enabling automatic compensation for manufacturing defects and environmental perturbations within milliseconds. Unlike traditional approaches limited by mechanical delay lines and atmospheric turbulence, our **tunable photonic architecture** achieves: (1) **sub-nanometer phase control** with thermal response times under 1 ms; (2) **intrinsic stability**, immune to vibrations and capable of compensating thermal drifts; (3) **scalable integration**, compatible with space missions and large ground-based arrays; and (4) potential **bandwidth adaptability** for spectral diversity.\n",
    "\n",
    "We demonstrate **two calibration algorithms** that optimize the 14-dimensional parameter space, retrieving an almost ideal Kernel-Nuller device. Advanced **statistical detection frameworks**, combining multiple kernel outputs, enable robust exoplanet characterization even in the presence of phase aberrations exceeding 100 nm RMS. Numerical simulations for **VLTI/ASGARD and LIFE mission architectures** validate detection capabilities for Earth-sized planets in habitable zones.\n",
    "\n",
    "This **paradigm shift toward adaptive photonic astronomy** opens new frontiers‚Äîfrom compact space-based interferometers to large-scale terrestrial arrays. Our approach provides the **technological foundation for the next generation of exoplanet discovery instruments**. The potential integration of machine learning with adaptive photonics positions this work at the intersection of cutting-edge technology and fundamental astrophysics, directly addressing the grand challenge of finding life beyond our solar system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# ‚ùó**Assumptions**\n",
    "\n",
    "</div>\n",
    "\n",
    "+ The banwidth $ŒîŒª$ is small enough to consider that:\n",
    "  - The flux is constant for all wavelengths $\\rightarrow F_Œª = F$\n",
    "  - The refractive index of the material is constant over the bandwidth $\\rightarrow n_Œª = n$ so phase shifters can be considered as pistons. We then express the phase shifts as OPDs (in distance unit). Justified [here](https://refractiveindex.info/?shelf=main&book=Si3N4&page=Luke).\n",
    "+ The star is not resolved so it can be associated to a point source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# ü§î **Context**\n",
    "\n",
    "</div>\n",
    "\n",
    "## üéØ Goal\n",
    "\n",
    "We aim to detect make direct detection of exoplanets. There is two main challenges to achieve this goal:\n",
    "- The **contrast**: the exoplanet is much fainter than the star it orbits. The contrast is typically of the order of $10^{-6}$ to $10^{-10}$.\n",
    "- The **angular separation**: the exoplanet is very close to the star. The angular separation depend on the distance of the exoplanet to the star and the distance of the star to the observer and can easily goes below the arcsecond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé Detection methods\n",
    "\n",
    "It exist several methods to detect exoplanets. The most common are:\n",
    "- **Radial velocity method**: the exoplanet induce a wobble in the star it orbits. This wobble can be detected by the Doppler effect (the light is alternatively redshifted and blueshifted).\n",
    "- **Transit method**: the exoplanet pass in front of the star and block a fraction of the light. This fraction can be detected by the decrease of the star luminosity.\n",
    "- **Microlensing**: the exoplanet act as a lens and magnify the light of a background star. This magnification can be detected by the increase of the star luminosity.\n",
    "- **Astrometry**: the exoplanet induce a wobble in the star it orbits. This wobble can be detected by the change of the star position.\n",
    "- **Coronography**: the exoplanet is directly imaged. This is the most challenging method because of the contrast and the angular separation.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"docs/img/detection_methods.jpg\" width=500px>\n",
    "<p><i>Paul Anthony Wilson - Exoplanet detection techniques</i><p>\n",
    "</div>\n",
    "\n",
    "Until now, the coronography was the only method allowing direct detection. But it has two main limitations:\n",
    "- It require huge telescopes in order to have a good angular resolution.\n",
    "- The contrast we can achieve is limited by unperfect fabrication process of the optical components which lead to undesired diffraction effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ûñ Nulling\n",
    "\n",
    "This is where the Nulling technic $^1$ come into play. The idea is to use two several telescopes and take advantage of destructives interferances to cancel the star light and using the fact that the planet is not perfectly in the line of sight, which will lead to an unperfect destructive interference, or in best scenarios, on constructive ones! This is a very challenging technic because it is highly phase sensitive and require a very good control of the optical path.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"docs/img/nulling_principle.jpg\" width=500px>\n",
    "</div>\n",
    "\n",
    "In a perfect 2-telescope nulling system, we can express the two acquired electric field (cf. section \"Signal nature\" below) in a vector:\n",
    "\n",
    "$$\n",
    "E = \\begin{pmatrix}\n",
    "E_1 \\\\\n",
    "E_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "And then express the nulling operation using the following matrix:\n",
    "\n",
    "$$\n",
    "N = \\begin{pmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & -1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Such as it gives us two outputs with a constructive and a destructive interference (we will focus on the latter):\n",
    "\n",
    "$$\n",
    "N \\cdot E = \\begin{pmatrix}\n",
    "E_1 + E_2 \\\\\n",
    "E_1 - E_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "> **References**\n",
    "> 1. Bracewell, R.N., MacPhie, R.H., 1979. Searching for nonsolar planets. Icarus 38, 136‚Äì147. https://doi.org/10.1016/0019-1035(79)90093-9\n",
    "\n",
    "cf. `mmi.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì° Projected telescope position\n",
    "\n",
    "The interferometry process depend on the projected geometry of the telescope position in the plane perpendicular to the line of sight. For each observation, we will then need to compute these projected positions in order to have the correct baseline lenght (and thus the correct phase shifts).\\\n",
    "These projected location can be computed using the following formula $^{1,2}$:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "u \\\\\n",
    "v\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "- \\sin(l) \\sin(h) & \\cos(h)\\\\\n",
    "\\sin(l) \\cos(h) \\sin(\\delta) + \\cos(l) \\cos(\\delta) & \\sin(h) \\sin(\\delta)\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "B_\\text{north} \\\\\n",
    "B_\\text{east}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "with\n",
    "- $l$ the latitude of the observatory\n",
    "- $h$ the hour angle\n",
    "- $\\delta$ the declination of the star\n",
    "\n",
    "> *Rerefence*\n",
    "> 1. Chingaipe, P.M. et al., 2023. High-contrast detection of exoplanets with a kernel-nuller at the VLTI. A&A 676, A43. https://doi.org/10.1051/0004-6361/202346118\n",
    "> 2. S√©gransan, D., 2007. Observability and UV coverage. New Astronomy Reviews 51, 597‚Äì603. https://doi.org/10.1016/j.newar.2007.06.005\n",
    "\n",
    "cf. `project_position_njit()` in `src.classes.context.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8f23426d0c4872a2809946c3bdf455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FloatSlider(value=-24.6275, description='Latitude (deg):', max=90.0, min=-90.0, step=0.01), Flo‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-24.6275 deg\n"
     ]
    }
   ],
   "source": [
    "from src import analysis\n",
    "from phise import Context\n",
    "\n",
    "ctx = Context.get_VLTI()\n",
    "analysis.projected_telescopes.gui(ctx, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÜ Signal nature\n",
    "\n",
    "The star and the planet are point sources. Seen from a classical telescope, it will result in an image made of the objects convolution with the point spread function (PSF) of the telescope.\n",
    "\n",
    "$$\n",
    "I = O \\otimes PSF\n",
    "$$\n",
    "\n",
    "Here we consider the most simple PSF : the Airy disk. The Airy disk is the diffraction pattern of a point source by a circular aperture. It is given by:\n",
    "\n",
    "$$\n",
    "PSF = \\left(\\frac{2J_1(x)}{x}\\right)^2\n",
    "$$\n",
    "\n",
    "where $J_1$ is the Bessel function of the first kind of order 1 and $x = \\frac{2\\pi r}{\\lambda f}$ is the normalized radius of the Airy disk.\n",
    "\n",
    "Then, we focus the image in a monomode optical fiber which will basically only keep the main lobe of the PSF and reshape it in a Gaussian form. In this process, we lose the spatial information so we have no longer images, but the light flux of each object in the fiber can be simply described by a complex number.\n",
    "\n",
    "Using this formalism, the light flux of the star and the planet can  be described by only 2 complex numbers for each telescope, giving the amplitude and phase of each object.\n",
    "\n",
    "cf. `signals.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîâ Photon noise\n",
    "\n",
    "The photon noise is the noise due to the quantization of the light in photons. It is a Poisson noise and can be described by the following formula:\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{N}\n",
    "$$\n",
    "\n",
    "where $N$ is the number of photons.\n",
    "\n",
    "The number of photons can be computed using the Light flux $F_\\lambda$ of the star and the planet, the collecting area $A$ of the telescope, the bandwidth $\\Delta \\lambda$, the transmission efficiency $\\eta$, the magnitude $M$ of the star, and the energy of a photon $E_\\nu$ at the frequency $\\nu$.\n",
    "\n",
    "Light flux is expressed in Jensky:\n",
    "$$\n",
    "1\\:\\text{Jensky} = 1\\:Jy = 10^{-26}\\:W/m^2/Hz = 10^{-26}\\:J/s/m^2/Hz\n",
    "$$\n",
    "The flux at zero magnitude $F_\\lambda$ for a star at different wavelengths is given by tables. The following one is an example for AB class stars such as Vega $^1$:\n",
    "\n",
    "| Band | $\\lambda$ ($\\mu m$) | $F_\\lambda$ ($Jy$) |\n",
    "|------|---------------------|--------------------|\n",
    "| V    | 0.55                | 3540               |\n",
    "| J    | 1.21                | 1630               |\n",
    "| H    | 1.65                | 1050               |\n",
    "| K    | 2.17                | 655                |\n",
    "| L    | 3.55                | 276                |\n",
    "\n",
    "The number of acquired photons per second is given by:\n",
    "\n",
    "$$\n",
    "N = F_\\lambda \\times A \\times \\Delta \\nu \\times \\eta \\times 10^{-\\frac{M}{2.5}} / E_\\nu\n",
    "$$\n",
    "\n",
    "where $A$ is the collecting area of the telescope, $\\Delta \\nu$ is the bandwidth, $\\eta$ is the transmission efficiency of the system, $M$ is the magnitude of the star, and $E_\\nu$ is the energy of a photon at the frequency $\\nu$.\n",
    "\n",
    "We can also express the bandwidth and eneergy in terms of the wavelength:\n",
    "$$\n",
    "\\Delta \\nu = \\frac{c}{\\lambda^2} \\Delta \\lambda\n",
    "$$\n",
    "$$\n",
    "E_\\nu = \\frac{h \\times c}{\\lambda}\n",
    "$$\n",
    "\n",
    "where $c$ is the speed of light, $h$ is the Planck constant.\n",
    "\n",
    "> **References**\n",
    "> 1. Allen's Astrophysical Quantities\n",
    "\n",
    "cf. `signals.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÆ MMI\n",
    "\n",
    "The nulling operation is made using Multi Mode Interferometer (MMI). It consist in a multimode waveguide taking several monomode fibers as input and output. The multimode waveguide is shaped in order to produce a specific interference operation, such as spreading the light of an input on all the output, or opposite, gathering the light of all the input on a single output.\n",
    "\n",
    "To design a simple nuller, we then need a 2x2 MMI that gather (ie. create a constructive interference) all the input light on a single output. The other output is then a \"nulled\" output, where there is actually all the inputs light but in phase opposition, resulting in a destructive interference.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"docs/img/mmi.png\" width=400px>\n",
    "\n",
    "*Numerical simulation of a 3x3 gathering MMI, taken from the paper of Cvetojevic et. al., 2022 $^1$*\n",
    "\n",
    "</div>\n",
    "\n",
    "> **Reference**\n",
    "> 1. Cvetojevic, N. et al., 2022. 3-beam self-calibrated Kernel nulling photonic interferometer. arXiv e-prints. https://doi.org/10.48550/arXiv.2206.04977"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ Recombiner\n",
    "\n",
    "The recombiner is also an MMI that will place the signals in phase quadrature. A particularity is that the output of the recombiner contain a symmetry. We will take advantage of this in the Kernel step.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"docs/img/recombiner.png\" width=500px>\n",
    "\n",
    "*Action of a 2x2 recombiner MMI, taking 2 different combination of 4 nulled signals as input. Taken from the paper of Romain Laugier et al., 2020 $^1$*\n",
    "\n",
    "</div>\n",
    "\n",
    "In a 2 input case, we can express the recombiner operation using the following matrix:\n",
    "\n",
    "$$  \n",
    "R = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}\n",
    "e^{i\\pi/4} & e^{-i\\pi/4} \\\\\n",
    "e^{-i\\pi/4} & e^{i\\pi/4}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "> **Reference**\n",
    "> 1. Laugier, R., Cvetojevic, N., Martinache, F., 2020. Kernel nullers for an arbitrary number of apertures. A&A 642, A202. https://doi.org/10.1051/0004-6361/202038866\n",
    "\n",
    "cf. `mmi.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí† Kernel\n",
    "\n",
    "The idea of the kernel is to acquire and substract the pairs of recombined output. As these pairs share symmetrical properties, this substraction will cancel the star light even with first order phase aberations while keeping the planet light!\n",
    "\n",
    "Moreover, it modify the nuller response (cf. \"Transmission maps\" section below) in an asymetric way which is interesting for us as it gives us more information to constrain the planet position.\n",
    "\n",
    "Demonstration (cf. `demonsration.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input intensities:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {I}_{a,p} + {I}_{a,s}$"
      ],
      "text/plain": [
       "I[a, p] + I[a, s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {I}_{b,p} + {I}_{b,s}$"
      ],
      "text/plain": [
       "I[b, p] + I[b, s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields contributions:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left|{{E}_{1,p} + {E}_{2,p} + {E}_{3,p} + {E}_{4,p}}\\right|^{2} + \\left|{{E}_{1,s} + {E}_{2,s} + {E}_{3,s} + {E}_{4,s}}\\right|^{2}$"
      ],
      "text/plain": [
       "Abs(E[1, p] + E[2, p] + E[3, p] + E[4, p])**2 + Abs(E[1, s] + E[2, s] + E[3, s] + E[4, s])**2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left|{{E}_{1,p} + {E}_{2,p} + {E}_{3,p} + {E}_{4,p}}\\right|^{2} + \\left|{{E}_{1,s} + {E}_{2,s} + {E}_{3,s} + {E}_{4,s}}\\right|^{2}$"
      ],
      "text/plain": [
       "Abs(E[1, p] + E[2, p] + E[3, p] + E[4, p])**2 + Abs(E[1, s] + E[2, s] + E[3, s] + E[4, s])**2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition in amplitudes and phases:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left(\\left(- i e^{i {\\phi}_{2}} {\\theta}_{2} - e^{i {\\phi}_{2}} - e^{i {\\phi}_{3}} {\\theta}_{3} + i e^{i {\\phi}_{3}} + e^{i {\\phi}_{4}} {\\theta}_{4} - i e^{i {\\phi}_{4}}\\right) e^{i \\left({\\phi}_{2} + {\\phi}_{3} + {\\phi}_{4}\\right)} {A}_{p}^{2} + \\left({A}_{p}^{2} {\\theta}_{2}^{2} + {A}_{p}^{2} {\\theta}_{3}^{2} + {A}_{p}^{2} {\\theta}_{4}^{2} + 4 {A}_{p}^{2} + {A}_{s}^{2} {\\theta}_{2}^{2} + {A}_{s}^{2} {\\theta}_{3}^{2} - 2 {A}_{s}^{2} {\\theta}_{3} {\\theta}_{4} + {A}_{s}^{2} {\\theta}_{4}^{2}\\right) e^{i \\left({\\phi}_{1} + {\\phi}_{2} + {\\phi}_{3} + {\\phi}_{4}\\right)} + \\left(- e^{i {\\phi}_{1}} {\\theta}_{3} - i e^{i {\\phi}_{1}} + i e^{i {\\phi}_{2}} {\\theta}_{2} {\\theta}_{3} - e^{i {\\phi}_{2}} {\\theta}_{2} + e^{i {\\phi}_{2}} {\\theta}_{3} + i e^{i {\\phi}_{2}} - e^{i {\\phi}_{4}} {\\theta}_{3} {\\theta}_{4} + i e^{i {\\phi}_{4}} {\\theta}_{3} - i e^{i {\\phi}_{4}} {\\theta}_{4} - e^{i {\\phi}_{4}}\\right) e^{i \\left({\\phi}_{1} + {\\phi}_{2} + {\\phi}_{4}\\right)} {A}_{p}^{2} + \\left(e^{i {\\phi}_{1}} {\\theta}_{4} + i e^{i {\\phi}_{1}} - i e^{i {\\phi}_{2}} {\\theta}_{2} {\\theta}_{4} + e^{i {\\phi}_{2}} {\\theta}_{2} - e^{i {\\phi}_{2}} {\\theta}_{4} - i e^{i {\\phi}_{2}} - e^{i {\\phi}_{3}} {\\theta}_{3} {\\theta}_{4} - i e^{i {\\phi}_{3}} {\\theta}_{3} + i e^{i {\\phi}_{3}} {\\theta}_{4} - e^{i {\\phi}_{3}}\\right) e^{i \\left({\\phi}_{1} + {\\phi}_{2} + {\\phi}_{3}\\right)} {A}_{p}^{2} + \\left(i e^{i {\\phi}_{1}} {\\theta}_{2} - e^{i {\\phi}_{1}} - i e^{i {\\phi}_{3}} {\\theta}_{2} {\\theta}_{3} - e^{i {\\phi}_{3}} {\\theta}_{2} + e^{i {\\phi}_{3}} {\\theta}_{3} - i e^{i {\\phi}_{3}} + i e^{i {\\phi}_{4}} {\\theta}_{2} {\\theta}_{4} + e^{i {\\phi}_{4}} {\\theta}_{2} - e^{i {\\phi}_{4}} {\\theta}_{4} + i e^{i {\\phi}_{4}}\\right) e^{i \\left({\\phi}_{1} + {\\phi}_{3} + {\\phi}_{4}\\right)} {A}_{p}^{2}\\right) e^{- i \\left({\\phi}_{1} + {\\phi}_{2} + {\\phi}_{3} + {\\phi}_{4}\\right)}$"
      ],
      "text/plain": [
       "((-I*exp(I*phi[2])*theta[2] - exp(I*phi[2]) - exp(I*phi[3])*theta[3] + I*exp(I*phi[3]) + exp(I*phi[4])*theta[4] - I*exp(I*phi[4]))*exp(I*(phi[2] + phi[3] + phi[4]))*A[p]**2 + (A[p]**2*theta[2]**2 + A[p]**2*theta[3]**2 + A[p]**2*theta[4]**2 + 4*A[p]**2 + A[s]**2*theta[2]**2 + A[s]**2*theta[3]**2 - 2*A[s]**2*theta[3]*theta[4] + A[s]**2*theta[4]**2)*exp(I*(phi[1] + phi[2] + phi[3] + phi[4])) + (-exp(I*phi[1])*theta[3] - I*exp(I*phi[1]) + I*exp(I*phi[2])*theta[2]*theta[3] - exp(I*phi[2])*theta[2] + exp(I*phi[2])*theta[3] + I*exp(I*phi[2]) - exp(I*phi[4])*theta[3]*theta[4] + I*exp(I*phi[4])*theta[3] - I*exp(I*phi[4])*theta[4] - exp(I*phi[4]))*exp(I*(phi[1] + phi[2] + phi[4]))*A[p]**2 + (exp(I*phi[1])*theta[4] + I*exp(I*phi[1]) - I*exp(I*phi[2])*theta[2]*theta[4] + exp(I*phi[2])*theta[2] - exp(I*phi[2])*theta[4] - I*exp(I*phi[2]) - exp(I*phi[3])*theta[3]*theta[4] - I*exp(I*phi[3])*theta[3] + I*exp(I*phi[3])*theta[4] - exp(I*phi[3]))*exp(I*(phi[1] + phi[2] + phi[3]))*A[p]**2 + (I*exp(I*phi[1])*theta[2] - exp(I*phi[1]) - I*exp(I*phi[3])*theta[2]*theta[3] - exp(I*phi[3])*theta[2] + exp(I*phi[3])*theta[3] - I*exp(I*phi[3]) + I*exp(I*phi[4])*theta[2]*theta[4] + exp(I*phi[4])*theta[2] - exp(I*phi[4])*theta[4] + I*exp(I*phi[4]))*exp(I*(phi[1] + phi[3] + phi[4]))*A[p]**2)*exp(-I*(phi[1] + phi[2] + phi[3] + phi[4]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left(\\left(- i e^{i {\\phi}_{2}} {\\theta}_{2} - e^{i {\\phi}_{2}} + e^{i {\\phi}_{3}} {\\theta}_{3} - i e^{i {\\phi}_{3}} - e^{i {\\phi}_{4}} {\\theta}_{4} + i e^{i {\\phi}_{4}}\\right) e^{i \\left({\\phi}_{2} + {\\phi}_{3} + {\\phi}_{4}\\right)} {A}_{p}^{2} + \\left({A}_{p}^{2} {\\theta}_{2}^{2} + {A}_{p}^{2} {\\theta}_{3}^{2} + {A}_{p}^{2} {\\theta}_{4}^{2} + 4 {A}_{p}^{2} + {A}_{s}^{2} {\\theta}_{2}^{2} + {A}_{s}^{2} {\\theta}_{3}^{2} - 2 {A}_{s}^{2} {\\theta}_{3} {\\theta}_{4} + {A}_{s}^{2} {\\theta}_{4}^{2}\\right) e^{i \\left({\\phi}_{1} + {\\phi}_{2} + {\\phi}_{3} + {\\phi}_{4}\\right)} + \\left(e^{i {\\phi}_{1}} {\\theta}_{3} + i e^{i {\\phi}_{1}} - i e^{i {\\phi}_{2}} {\\theta}_{2} {\\theta}_{3} + e^{i {\\phi}_{2}} {\\theta}_{2} - e^{i {\\phi}_{2}} {\\theta}_{3} - i e^{i {\\phi}_{2}} - e^{i {\\phi}_{4}} {\\theta}_{3} {\\theta}_{4} + i e^{i {\\phi}_{4}} {\\theta}_{3} - i e^{i {\\phi}_{4}} {\\theta}_{4} - e^{i {\\phi}_{4}}\\right) e^{i \\left({\\phi}_{1} + {\\phi}_{2} + {\\phi}_{4}\\right)} {A}_{p}^{2} + \\left(- e^{i {\\phi}_{1}} {\\theta}_{4} - i e^{i {\\phi}_{1}} + i e^{i {\\phi}_{2}} {\\theta}_{2} {\\theta}_{4} - e^{i {\\phi}_{2}} {\\theta}_{2} + e^{i {\\phi}_{2}} {\\theta}_{4} + i e^{i {\\phi}_{2}} - e^{i {\\phi}_{3}} {\\theta}_{3} {\\theta}_{4} - i e^{i {\\phi}_{3}} {\\theta}_{3} + i e^{i {\\phi}_{3}} {\\theta}_{4} - e^{i {\\phi}_{3}}\\right) e^{i \\left({\\phi}_{1} + {\\phi}_{2} + {\\phi}_{3}\\right)} {A}_{p}^{2} + \\left(i e^{i {\\phi}_{1}} {\\theta}_{2} - e^{i {\\phi}_{1}} + i e^{i {\\phi}_{3}} {\\theta}_{2} {\\theta}_{3} + e^{i {\\phi}_{3}} {\\theta}_{2} - e^{i {\\phi}_{3}} {\\theta}_{3} + i e^{i {\\phi}_{3}} - i e^{i {\\phi}_{4}} {\\theta}_{2} {\\theta}_{4} - e^{i {\\phi}_{4}} {\\theta}_{2} + e^{i {\\phi}_{4}} {\\theta}_{4} - i e^{i {\\phi}_{4}}\\right) e^{i \\left({\\phi}_{1} + {\\phi}_{3} + {\\phi}_{4}\\right)} {A}_{p}^{2}\\right) e^{- i \\left({\\phi}_{1} + {\\phi}_{2} + {\\phi}_{3} + {\\phi}_{4}\\right)}$"
      ],
      "text/plain": [
       "((-I*exp(I*phi[2])*theta[2] - exp(I*phi[2]) + exp(I*phi[3])*theta[3] - I*exp(I*phi[3]) - exp(I*phi[4])*theta[4] + I*exp(I*phi[4]))*exp(I*(phi[2] + phi[3] + phi[4]))*A[p]**2 + (A[p]**2*theta[2]**2 + A[p]**2*theta[3]**2 + A[p]**2*theta[4]**2 + 4*A[p]**2 + A[s]**2*theta[2]**2 + A[s]**2*theta[3]**2 - 2*A[s]**2*theta[3]*theta[4] + A[s]**2*theta[4]**2)*exp(I*(phi[1] + phi[2] + phi[3] + phi[4])) + (exp(I*phi[1])*theta[3] + I*exp(I*phi[1]) - I*exp(I*phi[2])*theta[2]*theta[3] + exp(I*phi[2])*theta[2] - exp(I*phi[2])*theta[3] - I*exp(I*phi[2]) - exp(I*phi[4])*theta[3]*theta[4] + I*exp(I*phi[4])*theta[3] - I*exp(I*phi[4])*theta[4] - exp(I*phi[4]))*exp(I*(phi[1] + phi[2] + phi[4]))*A[p]**2 + (-exp(I*phi[1])*theta[4] - I*exp(I*phi[1]) + I*exp(I*phi[2])*theta[2]*theta[4] - exp(I*phi[2])*theta[2] + exp(I*phi[2])*theta[4] + I*exp(I*phi[2]) - exp(I*phi[3])*theta[3]*theta[4] - I*exp(I*phi[3])*theta[3] + I*exp(I*phi[3])*theta[4] - exp(I*phi[3]))*exp(I*(phi[1] + phi[2] + phi[3]))*A[p]**2 + (I*exp(I*phi[1])*theta[2] - exp(I*phi[1]) + I*exp(I*phi[3])*theta[2]*theta[3] + exp(I*phi[3])*theta[2] - exp(I*phi[3])*theta[3] + I*exp(I*phi[3]) - I*exp(I*phi[4])*theta[2]*theta[4] - exp(I*phi[4])*theta[2] + exp(I*phi[4])*theta[4] - I*exp(I*phi[4]))*exp(I*(phi[1] + phi[3] + phi[4]))*A[p]**2)*exp(-I*(phi[1] + phi[2] + phi[3] + phi[4]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between the signals\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 \\left(\\left(- e^{i {\\phi}_{3}} {\\theta}_{3} + i e^{i {\\phi}_{3}} + e^{i {\\phi}_{4}} {\\theta}_{4} - i e^{i {\\phi}_{4}}\\right) e^{i \\left({\\phi}_{2} + {\\phi}_{3} + {\\phi}_{4}\\right)} + \\left(- e^{i {\\phi}_{1}} {\\theta}_{3} - i e^{i {\\phi}_{1}} + i e^{i {\\phi}_{2}} {\\theta}_{2} {\\theta}_{3} - e^{i {\\phi}_{2}} {\\theta}_{2} + e^{i {\\phi}_{2}} {\\theta}_{3} + i e^{i {\\phi}_{2}}\\right) e^{i \\left({\\phi}_{1} + {\\phi}_{2} + {\\phi}_{4}\\right)} + \\left(e^{i {\\phi}_{1}} {\\theta}_{4} + i e^{i {\\phi}_{1}} - i e^{i {\\phi}_{2}} {\\theta}_{2} {\\theta}_{4} + e^{i {\\phi}_{2}} {\\theta}_{2} - e^{i {\\phi}_{2}} {\\theta}_{4} - i e^{i {\\phi}_{2}}\\right) e^{i \\left({\\phi}_{1} + {\\phi}_{2} + {\\phi}_{3}\\right)} + \\left(- i e^{i {\\phi}_{3}} {\\theta}_{2} {\\theta}_{3} - e^{i {\\phi}_{3}} {\\theta}_{2} + e^{i {\\phi}_{3}} {\\theta}_{3} - i e^{i {\\phi}_{3}} + i e^{i {\\phi}_{4}} {\\theta}_{2} {\\theta}_{4} + e^{i {\\phi}_{4}} {\\theta}_{2} - e^{i {\\phi}_{4}} {\\theta}_{4} + i e^{i {\\phi}_{4}}\\right) e^{i \\left({\\phi}_{1} + {\\phi}_{3} + {\\phi}_{4}\\right)}\\right) e^{- i \\left({\\phi}_{1} + {\\phi}_{2} + {\\phi}_{3} + {\\phi}_{4}\\right)} {A}_{p}^{2}$"
      ],
      "text/plain": [
       "2*((-exp(I*phi[3])*theta[3] + I*exp(I*phi[3]) + exp(I*phi[4])*theta[4] - I*exp(I*phi[4]))*exp(I*(phi[2] + phi[3] + phi[4])) + (-exp(I*phi[1])*theta[3] - I*exp(I*phi[1]) + I*exp(I*phi[2])*theta[2]*theta[3] - exp(I*phi[2])*theta[2] + exp(I*phi[2])*theta[3] + I*exp(I*phi[2]))*exp(I*(phi[1] + phi[2] + phi[4])) + (exp(I*phi[1])*theta[4] + I*exp(I*phi[1]) - I*exp(I*phi[2])*theta[2]*theta[4] + exp(I*phi[2])*theta[2] - exp(I*phi[2])*theta[4] - I*exp(I*phi[2]))*exp(I*(phi[1] + phi[2] + phi[3])) + (-I*exp(I*phi[3])*theta[2]*theta[3] - exp(I*phi[3])*theta[2] + exp(I*phi[3])*theta[3] - I*exp(I*phi[3]) + I*exp(I*phi[4])*theta[2]*theta[4] + exp(I*phi[4])*theta[2] - exp(I*phi[4])*theta[4] + I*exp(I*phi[4]))*exp(I*(phi[1] + phi[3] + phi[4])))*exp(-I*(phi[1] + phi[2] + phi[3] + phi[4]))*A[p]**2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src import analysis\n",
    "analysis.demonstration.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå°Ô∏è Thermo-optic phase shifter\n",
    "\n",
    "In practice, we are often limited by the fabrication process of the optical components. The imperfections can lead into phase aberations that will degrade the Kernel-Nuller performance. An attempt to correct these aberations consist in using thermo-optic phase shifters. It consist in a waveguide with a heater that will modify the refractive index of the waveguide and thus the phase of the light passing through it.\n",
    "\n",
    "As the size of the waveguide is very small, the thermal inertia is very low and the phase can be modified very quickly, in a milisecond time scale. This is a very interesting solution to correct phase aberations, even in real time if we encounter variable phase aberation sources.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"docs/img/thermo-optic_phase_shifter.png\" width=500px>\n",
    "</div>\n",
    "\n",
    "In this simulation, one will simply modelize these phase shifter as a phase shift in the signal.\n",
    "\n",
    "$$\n",
    "P = e^{i\\phi} = e^{i\\frac{2\\pi}{\\lambda} \\Delta L}\n",
    "$$\n",
    "\n",
    "Where $\\phi$ and $\\Delta L$ are proportional to the power injected in the heater.\n",
    "\n",
    "cf. `phase.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# üí° **Our approach**\n",
    "\n",
    "</div>\n",
    "\n",
    "## üèóÔ∏è Current architecture\n",
    "\n",
    "To implement the 4 telescope tunable Kernel-Nuller, we splitted the 4x4 MMI into series of 2x2 MMI separated by phase shifters.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"docs/img/scheme.png\" width=1000px>\n",
    "\n",
    "*Architecture of our Kernel-Nuller. N squares are the 2x2 nullers, S squares are the 2x2 recombiners and P rectangles are the phase shifters*\n",
    "\n",
    "</div>\n",
    "\n",
    "cf. `kn.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Transmission maps\n",
    "\n",
    "The nulling technic with two telescope show a limitation: if the planet light arrive on the two telescopes with a phase shift of $2n\\pi$, the light will also be cancelled. It result in a comb-shaped transmission map $^1$, perpendicular to the baseline (there is clear bands where it's optimal to detect the planet and black bands where we will mostly destroy the planet light).\n",
    "\n",
    "The idea of Bracewell was to rotate the baseline in order to let the planet pass through the clear bands at some point. After an entire rotation of the baseline, we will have a sinusoidal signal from which the frequency will indicate us the distance of the planet to it's star, and the phase will give us a clue about the angle between the axes star-planet and the axes of the baseline. Thus, as the transmission map is symmetric, we can constrain the planet position to 2 possible locations, on both sides of the star.\n",
    "\n",
    "Here, we are using 4 telescopes, resulting in more complexe transmission maps than simple combs, but the principle is the same.\n",
    "\n",
    "> **Reference**\n",
    "> 1. Bracewell, R.N., MacPhie, R.H., 1979. Searching for nonsolar planets. Icarus 38, 136‚Äì147. https://doi.org/10.1016/0019-1035(79)90093-9\n",
    "\n",
    "cf. `transmission_map.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "from phise import Context\n",
    "from phise import Companion\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "\n",
    "ctx = Context.get_VLTI()\n",
    "ctx.Œì = 0 * u.m\n",
    "ctx.interferometer.chip.œÉ = np.zeros(14) * u.m\n",
    "\n",
    "# ctx.interferometer.Œª /= 5\n",
    "# ctx.interferometer.chip.Œª0 /= 5\n",
    "\n",
    "# ctx.target.companions[0].œÅ = 4.5 * u.mas\n",
    "# ctx.target.companions[0].Œ∏ = -49.5 * u.deg\n",
    "\n",
    "analysis.transmission_maps.gui(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# ü™õ **Calibration**\n",
    "\n",
    "</div>\n",
    "\n",
    "## ü´≥ Manual shift controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "\n",
    "# analysis.manual_control.gui(Œª=1.65 * u.um, œÜ=np.zeros(14) * u.nm, œÉ=np.abs(np.random.normal(0,100, 14)) * u.nm)\n",
    "analysis.manual_control.gui(Œª=1.65 * u.um, œÜ=np.zeros(14) * u.nm, œÉ=np.zeros(14) * u.nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùå Trial & Error\n",
    "\n",
    "in order to get the best shifts to inject to optimize the component performances, I made a genetic algorithme that iteratively mutate a shifter and keep the mutation if it minimize the associated metric. All the shifters that act on the bright channel are associated with the bright metric that must be maximized\n",
    "\n",
    "$$\n",
    "M_B = |B|^2\n",
    "$$\n",
    "\n",
    "While the other shifters are associated with the kernel metric that must be minimized.\n",
    "\n",
    "$$\n",
    "M_K = \\sum_{n=1}^3|K_n|\n",
    "$$\n",
    "\n",
    "Merging these two metrics can induce local minimum since improving the bright metric can deterior the kernel metric. This separation is then necessary to ensure reaching a global minimum (empirically demonstrated)\n",
    "\n",
    "> **Acknowledgment**\n",
    "> - Romain Laugier for the idea of merging the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "from phise import Context\n",
    "import astropy.units as u\n",
    "ctx = Context.get_LIFE()\n",
    "ctx.monochromatic = True\n",
    "_ = analysis.calibration.genetic_approach(ctx=ctx, Œ≤=0.961, verbose=False, figsize=(7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëÅÔ∏è Obstruction\n",
    "\n",
    "En obstruant successivement une paire d'entr√©es, il est possible de simplifier le probl√®me d'optimisation en ne jouant que sur un seul param√®tre et en ne regardant qu'une seule sortie.\n",
    "\n",
    "Il existe diff√©rentes fa√ßon de proc√©der. Je ne vais d√©tailler ici que l'une d'entre elles.\n",
    "\n",
    "On commence par obstruer les entr√©es $I_2$ et $I_3$. Au regard de l'architectue de notre composant, on peut alors d√©crire la fonction de transfert pour la sortie brillante $B$\n",
    "\n",
    "$$\n",
    "B = \\left|\\left(a_1 e^{i(\\theta_1 + \\sigma_1 + \\phi_1)} + a_2 e^{i(\\theta_2 + \\sigma_2 + \\phi_2)}\\right) e^{i(\\sigma_5 + \\phi_5)}\\right|^2\n",
    "$$\n",
    "\n",
    "O√π $a_n$ et $\\theta_n$ repr√©sentent respectivement l'amplitude et la phase des signaux d'entr√©e. $\\sigma_n$ correspond √† la perturbation de phase (inconnue) associ√© au retardateur $n$ et $\\phi_n$ est la phase que l'on inject volontairement via le retardateur pour tenter de compenser cette perturbation.\n",
    "\n",
    "La calibration se faisant en laboatoire, on peut supposer une intensit√© totale fix√©e √† $1$ (unit√© arbitraire) et que chaque entr√©e rec√ßoi le m√™me flux soit $a_1 = a_2 = 1/\\sqrt{2}$, et parfaitement cophas√©, soit $\\theta_1 = \\theta_2 = \\theta$. Etant donn√© que l'on a acc√®s qu'a l'intensit√© du signal, nous sommes insensible √† la phase globale, ce qui permet de simplifier l'√©quation pr√©c√©dente :\n",
    "\n",
    "$$\n",
    "B = \\frac{1}{2} \\left|e^{i(\\sigma_1 + \\phi_1)} + e^{i(\\sigma_2 + \\phi_2)}\\right|^2\n",
    "$$\n",
    "\n",
    "En maximisant $B$, on devrait alors trouver $1$ ce qui implique que\n",
    "\n",
    "$$\n",
    "\\sigma_1 + \\phi_1 = \\sigma_2 + \\phi_2\n",
    "$$\n",
    "\n",
    "On peut utiliser $\\phi_1$ comme r√©f√©rence (phase globale) et ainsi le fixer √† 0, ce qui donne alors\n",
    "\n",
    "$$\n",
    "\\phi_2 = \\sigma_1 - \\sigma_2\n",
    "$$\n",
    "\n",
    "On peut alors soit effectuer diff√©rentes mesures de $B$ √† $\\phi_2$ fix√© et en d√©duire $\\sigma_1$ et $\\sigma_2$ par r√©solution d'un syst√®me d√©quation, soit trouver dichotomiquement la valeur de $\\phi_2$ qui maximise $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "from phise import Context\n",
    "import astropy.units as u\n",
    "\n",
    "ctx = Context.get_VLTI()\n",
    "ctx.monochromatic = False\n",
    "ctx.interferometer.ŒîŒª = 0.1 * u.um\n",
    "_ = analysis.calibration.obstruction_approach(n=1000, ctx=ctx, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßê Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "analysis.calibration.compare_approaches(Œ≤=0.99, n=10_000, figsize=(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Machine Learning\n",
    "\n",
    "Another approach to retrieve the correct shifts inject is to use machine learning techniques. There is several ways to do so. Here we will focus a supervised dense neural network. To do so, we will have to build a dataset.\n",
    "\n",
    "As the solutions are degenerated, we will not ask the network to predict the best shift to inject, but we will ask it to predict the shfit aberrations instead. From these aberation, we are able to determine a solution for the shifts to inject.\n",
    "\n",
    "As input of the network, we need to give enough information to caracterize the parameter space. The most straightforward approach would be to create a grid in the parameter space and give the kenrel outputs for each of the point in these grid.\n",
    "\n",
    "Unfortunately, we work in a parameter space of 14 dimensions which is too large to be covered by a grid. A solution is to consider only the vectors that form the cardinal basis of this parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor the following code\n",
    "# DATASET = ml.get_dataset(10_000)\n",
    "# print(DATASET.shape)\n",
    "# MODEL = ml.get_model(input_shape=DATASET.shape[1]-14)\n",
    "# MODEL.summary()\n",
    "# ml.train_model(plot=True)\n",
    "# ml.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõú Wavelenght scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "\n",
    "analysis.wavelenght_scan.run(figsize=(4,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# ‚öôÔ∏è **Data generation**\n",
    "\n",
    "</div>\n",
    "\n",
    "First, let's generate the different scenes we will compare:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü® Instantaneous serie\n",
    "\n",
    "Monochromatic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "from phise import Context\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "\n",
    "ctx = Context.get_VLTI()\n",
    "ctx.interferometer.kn.œÉ = np.zeros(14) * u.nm\n",
    "ctx.target.companions[0].c = 1e-2\n",
    "ctx.monochromatic = True\n",
    "ctx.interferometer.ŒîŒª = 0.01 * u.um\n",
    "_ = analysis.data_representations.instant_distribution(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polychromatic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "from phise import Context\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "\n",
    "ctx = Context.get_VLTI()\n",
    "ctx.interferometer.kn.œÉ = np.zeros(14) * u.nm\n",
    "ctx.target.companions[0].c = 1e-2\n",
    "ctx.monochromatic = False\n",
    "ctx.interferometer.ŒîŒª = 0.01 * u.um\n",
    "\n",
    "_ = analysis.data_representations.instant_distribution(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü° Time serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "from phise import Context\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "from phise import Companion\n",
    "\n",
    "ctx = Context.get_VLTI()\n",
    "ctx.interferometer.Œª = 1.65 * u.um\n",
    "ctx.interferometer.kn.œÉ = np.zeros(14) * u.nm\n",
    "ctx.target.companions = []\n",
    "ctx.monochromatic = True\n",
    "ctx.target.companions.append(\n",
    "    Companion(\n",
    "        c=1e-2,\n",
    "        œÅ=2 * u.mas,\n",
    "        Œ∏=3*np.pi/4 * u.rad,\n",
    "    )\n",
    ")\n",
    "\n",
    "# ctx.target.companions.append(\n",
    "#     Companion(\n",
    "#         c=1e-8,\n",
    "#         œÅ=8 * u.mas,\n",
    "#         Œ∏=0 * u.rad,\n",
    "#     )\n",
    "# )\n",
    "# ctx.calibrate_obs(1000)\n",
    "\n",
    "data, data_ref = analysis.data_representations.time_evolution(ctx, n=60, map=np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "analysis.temporal_response.gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "from phise import Context\n",
    "\n",
    "ctx = Context.get_LIFE()\n",
    "\n",
    "analysis.temporal_response.fit(ctx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# üîé **Data analysis**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¢ Noise sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "from phise import Context\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "ctx = Context.get_VLTI()\n",
    "ctx.monochromatic = True\n",
    "ctx.interferometer.kn.œÉ = (np.random.random(14)*2-1) * ctx.interferometer.Œª / 10\n",
    "analysis.noise_sensitivity.plot(ctx=ctx, Œ≤=0.961, n=1_000, figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òëÔ∏è Model fitting (WIP)\n",
    "\n",
    "Distributions are cool, but in order to make deeper analysis, we want to find a model that describe these distribution using few parameters. Unfortunately, there is no straightforward way to get such model as these distribution are very particular.\n",
    "\n",
    "The next block try most of the common distribution models and show the best ones... but unfortunately, none of them seems to match üòû"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "from phise import Context\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "\n",
    "ctx = Context.get_VLTI()\n",
    "# ctx.observe = ctx.observe_monochromatic\n",
    "ctx.target.companions = []\n",
    "ctx.interferometer.kn.œÉ = np.zeros(14) * u.nm\n",
    "ctx.monochromatic = True\n",
    "\n",
    "analysis.distribution_model.fit(ctx = ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Statistical model\n",
    "\n",
    "Dans cette section, on va √©tudier comment se comportent les donn√©es en sortie d'un Kernel-Nuller afin d'en extraire une description analytique qui nous sera utile pour construire des tests statistiques adapt√©s.\n",
    "\n",
    "**Hypoth√®ses de travail**\n",
    "- On se place dans le cas d'un √©toile seule ou accompagn√©e d'une seule plan√®te. Toute autre source est ignor√©e.\n",
    "\n",
    "**Ansatz**\n",
    "- Les donn√©es peuvent √™tre exprim√©es sous la forme:\n",
    "\n",
    "    $$\n",
    "    x_i = \\alpha S_p + n_* + n_p\n",
    "    $$\n",
    "\n",
    "    With\n",
    "    - $\\hat{S_p}$ the signal of the planet\n",
    "    - $\\alpha$ is a coefficient that scales this signal (instrumental response). It lives in $[-1,1]$\n",
    "    - $n_*$ is the noise term from the star (starlight leakage)\n",
    "    - $n_p$ is the noise term from the planet (position drifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data gen\n",
    "\n",
    "from src import analysis\n",
    "from phise import Context\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "\n",
    "def get_ctx() -> Context:\n",
    "    ctx = Context.get_VLTI()\n",
    "    ctx.target.companions[0].c = 1e-2\n",
    "    ctx.interferometer.kn.œÉ = np.zeros(14) * u.nm\n",
    "    ctx.monochromatic = True\n",
    "    return ctx\n",
    "\n",
    "N = 10_000\n",
    "def get_distrib(ctx) -> np.ndarray:\n",
    "    dists = np.empty((3, N))\n",
    "    for i in range(N):\n",
    "        print(f\"‚åõ Sampling {i+1}/{N} ({(i+1)/N:.2%})\", end='\\r')\n",
    "        # observe() returns raw intensities; process to get kernels\n",
    "        outs = ctx.observe()\n",
    "        k = ctx.interferometer.chip.process_outputs(outs)\n",
    "        dists[:, i] = k\n",
    "    print(\"‚úÖ Done\" + \" \" * 30)\n",
    "    return dists\n",
    "\n",
    "ctx = get_ctx()\n",
    "\n",
    "ctx.plot_transmission_maps(N=100)\n",
    "\n",
    "ctx.target.companions = []\n",
    "dists_so = get_distrib(ctx)\n",
    "\n",
    "ctx = get_ctx()\n",
    "print(\"Before scaling:\")\n",
    "print(\"    Star flux:\", ctx.target.f)\n",
    "print(\"    Companion flux:\", ctx.target.companions[0].c * ctx.target.f)\n",
    "scale = 1e12\n",
    "ctx.target.f /= scale\n",
    "ctx.target.companions[0].c *= scale\n",
    "print(\"After scaling:\")\n",
    "print(\"    Star flux:\", ctx.target.f)\n",
    "print(\"    Companion flux:\", ctx.target.companions[0].c * ctx.target.f)\n",
    "dists_po = get_distrib(ctx)\n",
    "\n",
    "ctx = get_ctx()\n",
    "dists_full = get_distrib(ctx)\n",
    "\n",
    "dists_comb = dists_so + dists_po\n",
    "\n",
    "dists_star_noise = np.empty_like(dists_so)\n",
    "for k in range(3):\n",
    "    dists_star_noise[k] = dists_so[k] + np.median(dists_po[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "kmin = 0\n",
    "kmax = 0\n",
    "for dist in [dists_so, dists_po, dists_full, dists_comb, dists_star_noise]:\n",
    "    for k in range(3):\n",
    "        mi, ma = np.percentile(dist, [5, 95])\n",
    "        kmin = min(kmin, mi)\n",
    "        kmax = max(kmax, ma)\n",
    "\n",
    "for k in range(3):\n",
    "    # Use 2*sqrt(samples) as number of bins\n",
    "    bins = np.linspace(kmin, kmax, 2*int(np.sqrt(N)) + 1)\n",
    "\n",
    "\n",
    "# Plot histograms to compare distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(15, 8), tight_layout=True)\n",
    "labels = ['k1', 'k2', 'k3']\n",
    "for i in range(3):\n",
    "    axs[i].hist(dists_so[i], bins=bins, alpha=0.5, label='Star Only', density=True, log=True)\n",
    "    axs[i].set_title(f'Distribution on Kernel {i+1}')\n",
    "    axs[i].set_xlabel('Intensity [photons events]')\n",
    "    axs[i].set_ylabel('Occurences')\n",
    "    axs[i].legend()\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(15, 8), tight_layout=True)\n",
    "labels = ['k1', 'k2', 'k3']\n",
    "for i in range(3):\n",
    "    axs[i].hist(dists_po[i], bins=bins, alpha=0.5, label='Planet Only', density=True, log=True)\n",
    "    axs[i].set_title(f'Distribution on Kernel {i+1}')\n",
    "    axs[i].set_xlabel('Intensity [photons events]')\n",
    "    axs[i].set_ylabel('Occurences')\n",
    "    axs[i].legend()\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(15, 8), tight_layout=True)\n",
    "labels = ['k1', 'k2', 'k3']\n",
    "for i in range(3):\n",
    "    axs[i].hist(dists_full[i], bins=bins, alpha=0.5, label='Full model', density=True, log=True)\n",
    "    axs[i].hist(dists_comb[i], bins=bins, alpha=0.5, label=r'$x = n_* + S_p + n_p$', histtype='step', linewidth=2, density=True, log=True)\n",
    "    axs[i].hist(dists_star_noise[i], bins=bins, alpha=0.5, label=r'$x = n_* + S_p$', histtype='step', linewidth=2, density=True, log=True)\n",
    "    axs[i].set_title(f'Distribution on Kernel {i+1}')\n",
    "    axs[i].set_xlabel('Intensity [photons events]')\n",
    "    axs[i].set_ylabel('Occurences')\n",
    "    axs[i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Test statistics on distributions\n",
    "\n",
    "A test statistic is a way of reducing the data we have to an unique number and compare this number to a threshold value. If the number is below the treshold, then the null hypothesis is favored. If it is above, the alternative hypothesis is favored. The goal is to find the best test statistic that allow to distinguish both hypothesis in a correct way\n",
    "\n",
    "- $H0$: the null hypothesis -> there is no planet\n",
    "- $H1$: the alternative hypothesis -> there is a planet\n",
    "\n",
    "- $T0$: vector of distributions obtained with H0\n",
    "- $T1$: vector of distributions obtained with H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phise.modules.test_statistics as ts\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "\n",
    "from src import analysis\n",
    "from phise import Context\n",
    "\n",
    "ctx = Context.get_VLTI()\n",
    "\n",
    "ctx.interferometer.kn.œÉ = np.zeros(14) * u.nm\n",
    "ctx.target.companions[0].c = 1e-2\n",
    "\n",
    "ctx.monochromatic = True\n",
    "\n",
    "T0, T1 = ts.get_vectors(ctx=ctx, nmc=100, size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy as copy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import laplace, cauchy\n",
    "\n",
    "def get_dists(ctx, N=1000):\n",
    "    dists = np.empty((3, N))\n",
    "    for i in range(N):\n",
    "        # observe() returns raw intensities; process to get kernels and normalize by bright\n",
    "        outs = ctx.observe()\n",
    "        k = ctx.interferometer.chip.process_outputs(outs)\n",
    "        b = outs[0]\n",
    "        k = k / b\n",
    "        dists[:, i] = k\n",
    "    return dists\n",
    "\n",
    "def get_sigma(x, Œº):\n",
    "    mads = np.empty(3)\n",
    "    for k in range(3):\n",
    "        mad = np.median(np.abs(x[k, :] - Œº[k]))\n",
    "        b_mad = mad / np.log(2)\n",
    "        sigma_mad = np.sqrt(2) * b_mad\n",
    "        mads[k] = sigma_mad\n",
    "    return mads\n",
    "\n",
    "# Ideal case ------------------------------------------------------------------\n",
    "\n",
    "tmp_ctx = copy(ctx)\n",
    "tmp_ctx.monochromatic = False\n",
    "tmp_ctx.Œì = 0 * u.m\n",
    "tmp_ctx.interferometer.camera.ideal = True\n",
    "Œº1 = get_dists(tmp_ctx, N=1)[:,0]\n",
    "tmp_ctx.target.companions = []\n",
    "Œº0 = get_dists(tmp_ctx, N=1)[:,0]\n",
    "\n",
    "# Realistic case --------------------------------------------------------------\n",
    "\n",
    "tmp_ctx = copy(ctx)\n",
    "tmp_ctx.monochromatic = False\n",
    "\n",
    "k1 = get_dists(tmp_ctx)\n",
    "tmp_ctx.target.companions = []\n",
    "k0 = get_dists(tmp_ctx)\n",
    "\n",
    "# Dispersion\n",
    "œÉ0 = get_sigma(k0, Œº0)\n",
    "œÉ1 = get_sigma(k1, Œº1)\n",
    "\n",
    "# Plotting --------------------------------------------------------------------\n",
    "\n",
    "lims = np.max(np.abs(Œº1)) + 5*np.max(œÉ1)\n",
    "x = np.linspace(-lims, lims, 1000)\n",
    "\n",
    "for k in range(3):\n",
    "    laplace_pdf0 = laplace.pdf(x, loc=Œº0[k], scale=œÉ0[k])\n",
    "    laplace_pdf1 = laplace.pdf(x, loc=Œº1[k], scale=œÉ1[k])\n",
    "    plt.plot(x, laplace_pdf0, label='Laplace H0', color='blue')\n",
    "    plt.plot(x, laplace_pdf1, label='Laplace H1', color='red')\n",
    "    cauchy_pdf0 = cauchy.pdf(x, loc=Œº0[k], scale=œÉ0[k])\n",
    "    cauchy_pdf1 = cauchy.pdf(x, loc=Œº1[k], scale=œÉ1[k])\n",
    "    plt.plot(x, cauchy_pdf0, label='Cauchy H0', color='cyan')\n",
    "    plt.plot(x, cauchy_pdf1, label='Cauchy H1', color='magenta')\n",
    "    plt.hist(k0[k,:], bins=100, density=True, alpha=0.5, color='blue', log=False)\n",
    "    plt.hist(k1[k,:], bins=100, density=True, alpha=0.5, color='red', log=False)\n",
    "    plt.legend()\n",
    "    plt.title(f'Kernel {k+1}')\n",
    "    plt.xlim(-lims, lims)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean\n",
    "\n",
    "**Principle:** We take the average of the distribution and we compare it to a treshold.\n",
    "\n",
    "$$\n",
    "\\left|\\frac{1}{N}\\sum_i x_i \\right| \\stackrel{H_1}{\\underset{H_0}{\\gtrless}} \\xi\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "H_0 : d = |\\bar{n}|\\\\\n",
    "H_1 : d =  |\\alpha \\hat{S_p} + \\bar{n}|\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median\n",
    "\n",
    "**Principle:** We take the median of the distribution and we compare it to a treshold.\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\left| x_{\\frac{N+1}{2}} \\right| & \\text{if }N\\text{ is odd} \\\\\n",
    "\n",
    "\\left| \\frac{x_{\\frac{N}{2}} + x_{\\frac{N+1}{2}}}{2} \\right|  & \\text{if }N\\text{ is odd}\n",
    "\\end{cases}\n",
    "\\quad\\stackrel{H_1}{\\underset{H_0}{\\gtrless}} \\xi\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "H_0 : d = |\\tilde{n}|\\\\\n",
    "H_1 : d =  | \\alpha \\hat{S_p} + \\tilde{n} |\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argmax\n",
    "\n",
    "**Principle:** We pack our data in bins and we consider the position of the bin with the highest number of occurences. We compare it to a treshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov\n",
    "\n",
    "**Principle:** We compare the maximum distance on the cumulative distribution functions of the two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cram√®r-von Mises\n",
    "\n",
    "**Principle:** We compare the total quadratique distance on the cumulative distribution functions of the two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wilcoxon-Mann-Whitney (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDF diff area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} |x_i - \\tilde{x}| \\stackrel{H_1}{\\underset{H_0}{\\gtrless}} \\xi\n",
    "$$\n",
    "\n",
    "with $\\tilde{x} = \\text{median}(x)$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "H_0 : d = \\sum |n - \\tilde{n}|\\\\\n",
    "H_1 : d = \\sum \\frac{|n - \\tilde{n}|}{\\beta(\\hat{S_p})}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median of abs\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "H_0 : d = \\tilde{\\text{abs}(n)}\\\\\n",
    "H_1 : d = \\alpha \\times \\hat{\\text{abs}(S_p)} + \\frac{\\tilde{\\text{abs}(n)}}{\\beta(\\hat{S_p})}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì ROC curves\n",
    "\n",
    "ROC curves allow to compare the power of different test statistics. It show the proportion of true detection in function of the probability of false alarm. The more the curve climb fast, the better it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis import distrib_test_statistics as ts\n",
    "ts.plot_rocs(t0=T0, t1=T1, figsize=(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™ Tests power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phise import Context\n",
    "import src.analysis.distrib_test_statistics as ts\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "\n",
    "ctx = Context.get_LIFE()\n",
    "ctx.interferometer.kn.œÉ = np.zeros(14) * u.nm\n",
    "ctx.target.companions[0].c = 1e-9\n",
    "\n",
    "ts.test_power(ctx=ctx, bootstrap=3, nmc=100, resolution=10, maxpoints=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phise import Context\n",
    "import src.analysis.distrib_test_statistics as ts\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "\n",
    "ctx = Context.get_VLTI()\n",
    "ctx.interferometer.kn.œÉ = np.zeros(14) * u.nm\n",
    "ctx.target.companions[0].c = 1e-2\n",
    "\n",
    "ts.test_power(ctx=ctx, bootstrap=3, nmc=100, resolution=10, maxpoints=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.analysis.distrib_test_statistics as ts\n",
    "from phise import Context\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ctx = Context.get_LIFE()\n",
    "ctx.interferometer.kn.œÉ = np.zeros(14) * u.nm\n",
    "ctx.target.companions[0].c = 1e-3\n",
    "\n",
    "size = 1000\n",
    "t0, t1 = ts.get_vectors(ctx=ctx, nmc=10, size=size)\n",
    "\n",
    "x = np.linspace(0, 1, size)\n",
    "\n",
    "def compute(t):\n",
    "    shifts = np.median(t, axis=1)\n",
    "    mean_shift = np.mean(shifts, axis=0)\n",
    "    std_shift = np.std(shifts, axis=0)\n",
    "\n",
    "    distances = np.empty_like(t)\n",
    "    for i in range(t.shape[0]):\n",
    "        distances[i] = np.sort(np.abs(t[i] - shifts[i]))\n",
    "    mean_distances = np.median(distances, axis=0)\n",
    "    std_distances = np.std(distances, axis=0)\n",
    "\n",
    "    comb = distances.copy()\n",
    "    for i in range(t.shape[0]):\n",
    "        comb[i] += np.abs(shifts[i])\n",
    "    mean_comb = np.mean(comb, axis=0)\n",
    "    std_comb = np.std(comb, axis=0)\n",
    "\n",
    "    return mean_shift, mean_distances, mean_comb, std_shift, std_distances, std_comb\n",
    "\n",
    "# H0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "s0, d0, c0, œÉs0, œÉd0, œÉc0 = compute(t0)\n",
    "\n",
    "# plt.plot(x, np.ones(len(x)) * s0, label='Median H0', linestyle='--', color='orange')\n",
    "# plt.fill_between(x, s0 - œÉs0, s0 + œÉs0, color='orange', alpha=0.2)\n",
    "\n",
    "plt.plot(x, d0, label='Dist H0', color='orange')\n",
    "plt.fill_between(x, d0 - œÉd0, d0 + œÉd0, color='orange', alpha=0.2)\n",
    "\n",
    "plt.plot(x, c0, label='Comb H0', color='red')\n",
    "plt.fill_between(x, c0 - œÉc0, c0 + œÉc0, color='red', alpha=0.2)\n",
    "\n",
    "# H1 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "s1, d1, c1, œÉs1, œÉd1, œÉc1 = compute(t1)\n",
    "\n",
    "# plt.plot(x, np.ones(len(x)) * s1, label='Median H1', linestyle='--', color='aqua')\n",
    "# plt.fill_between(x, s1 - œÉs1, s1 + œÉs1, color='aqua', alpha=0.2)\n",
    "\n",
    "plt.plot(x, d1, label='Dist H1', color='aqua')\n",
    "plt.fill_between(x, d1 - œÉd1, d1 + œÉd1, color='aqua', alpha=0.2)\n",
    "\n",
    "plt.plot(x, c1, label='Comb H1', color='blue')\n",
    "plt.fill_between(x, c1 - œÉc1, c1 + œÉc1, color='blue', alpha=0.2)\n",
    "\n",
    "# Plot parameters ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "plt.title('Distance to median')\n",
    "plt.xlabel('Normalized rank')\n",
    "plt.ylabel('Distance')\n",
    "plt.legend()\n",
    "# Logscale\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Likelihood Ratio\n",
    "\n",
    "$$\n",
    "\\Lambda(x) = \\frac{p(x|H_0)}{p(x|H_1)}\n",
    "$$\n",
    "\n",
    "Where $p(x|H_0)$ and $p(x|H_1)$ are the probability density functions of the data $x$ under the null hypothesis $H_0$ and the alternative hypothesis $H_1$ respectively.\n",
    "\n",
    "### Gaussian case\n",
    "\n",
    "If we consider that the data follow a Gaussian distribution :\n",
    "\n",
    "$$\n",
    "p(x;H_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma_i} \\exp\\left(-\\frac{(x - \\mu_i)^2}{2\\sigma_i^2}\\right)\n",
    "$$\n",
    "\n",
    "we can express the likelihood ratio as:\n",
    "\n",
    "$$\n",
    "\\Lambda(\\vec{x}) = \\prod_{i=1}^{n} \\frac{\\sigma_1}{\\sigma_0} \\exp\\left(\\frac{(x_i - \\mu_1)^2}{2\\sigma_1^2} - \\frac{(x_i - \\mu_0)^2}{2\\sigma_0^2}\\right)\\\\\n",
    "= \\left(\\frac{\\sigma_1}{\\sigma_0}\\right)^n \\exp\\left(\\sum_{i=1}^{n} \\frac{(x_i - \\mu_1)^2}{2\\sigma_1^2} - \\frac{(x_i - \\mu_0)^2}{2\\sigma_0^2}\\right)\n",
    "$$\n",
    "\n",
    "If we take the log and remove the constant term, we get:\n",
    "\n",
    "$$\n",
    "\\log(\\Lambda(\\vec{x})) \\propto \\sum_{i=1}^{n} \\frac{(x_i - \\mu_1)^2}{2\\sigma_1^2} - \\frac{(x_i - \\mu_0)^2}{2\\sigma_0^2}\n",
    "$$\n",
    "\n",
    "Where $\\mu_0$ and $\\sigma_0$ are the mean and standard deviation of the distribution under $H_0$, and $\\mu_1$ and $\\sigma_1$ are the mean and standard deviation of the distribution under $H_1$.\n",
    "\n",
    "### Laplacian case\n",
    "\n",
    "If we consider that the data follow a Laplacian distribution :\n",
    "\n",
    "$$\n",
    "p(x;H_i) = \\frac{1}{2b_i} \\exp\\left(-\\frac{|x - \\mu_i|}{b_i}\\right)\n",
    "$$\n",
    "\n",
    "we can express the likelihood ratio as:\n",
    "\n",
    "$$\n",
    "\\Lambda(\\vec{x}) = \\prod_{i=1}^{n} \\frac{b_1}{b_0} \\exp\\left(\\frac{|x_i - \\mu_1|}{b_1} - \\frac{|x_i - \\mu_0|}{b_0}\\right)\\\\\n",
    "= \\left(\\frac{b_1}{b_0}\\right)^n \\exp\\left(\\sum_{i=1}^{n} \\frac{|x_i - \\mu_1|}{b_1} - \\frac{|x_i - \\mu_0|}{b_0}\\right)\n",
    "$$\n",
    "\n",
    "If we take the log and remove the constant term, we get:\n",
    "\n",
    "$$\n",
    "\\log(\\Lambda(\\vec{x})) \\propto \\sum_{i=1}^{n} \\frac{|x_i - \\mu_1|}{b_1} - \\frac{|x_i - \\mu_0|}{b_0}\n",
    "$$\n",
    "\n",
    "Where $\\mu_0$ and $b_0$ are the mean and scale parameter of the distribution under $H_0$, and $\\mu_1$ and $b_1$ are the mean and scale parameter of the distribution under $H_1$.\n",
    "\n",
    "### Cauchy case\n",
    "\n",
    "If we consider that the data follow a Cauchy distribution :\n",
    "\n",
    "$$\n",
    "p(x;H_i) = \\frac{1}{\\pi \\gamma_i \\left[1 + \\left(\\frac{x - x_i}{\\gamma_i}\\right)^2\\right]}\n",
    "$$\n",
    "\n",
    "We can express the likelihood ratio as:\n",
    "\n",
    "$$\n",
    "\\Lambda(\\vec{x}) = \\prod_{i=1}^{n} \\frac{\\gamma_1(1+(\\frac{x_i - x_1}{\\gamma_1})^2)}{\\gamma_0(1+(\\frac{x_i - x_0}{\\gamma_0})^2)}\\\\\n",
    "= \\left(\\frac{\\gamma_1}{\\gamma_0}\\right)^n \\prod_{i=1}^{n} \\frac{1+(\\frac{x_i - x_1}{\\gamma_1})^2}{1+(\\frac{x_i - x_0}{\\gamma_0})^2}\n",
    "$$\n",
    "\n",
    "If we consider the log and remove the constant term, we get:\n",
    "\n",
    "$$\n",
    "\\log(\\Lambda(\\vec{x})) \\propto \\sum_{i=1}^{n} \\log\\left(\\frac{1+(\\frac{x_i - x_1}{\\gamma_1})^2}{1+(\\frac{x_i - x_0}{\\gamma_0})^2}\\right)\n",
    "$$\n",
    "\n",
    "Where $x_0$ and $\\gamma_0$ are the location and scale parameters of the distribution under $H_0$, and $x_1$ and $\\gamma_1$ are the location and scale parameters of the distribution under $H_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.analysis.distrib_test_statistics as ts\n",
    "ts.np_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úîÔ∏è P-values\n",
    "\n",
    "The P-value is an indicator of the confidence we have to reject the null hypothesis.\n",
    "\n",
    "The principle consist in comparing the test statistic obtain on the data we want to test with a large bunch of data that we know to be under the null hypothesis. We then compute the proportion of test statistic that are above the one we obtained. This proportion is the P-value.\n",
    "\n",
    "Thus, the lower the P-value, the more confident we are to reject the null hypothesis. A P-value below 0.05 is commonly considered as a good indicator to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.plot_p_values(t0=T0, t1=T1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# ü™ê **Characterization**\n",
    "\n",
    "</div>\n",
    "\n",
    "## ‚ôí Modulation fit\n",
    "\n",
    "In order to determine the planet position, we need to rotate the interferometer baseline in order to rotate the transmission map. Thus, the planet signal will be modulated. By analysis this modulation (trying to fit the parametrized modulation function to the data points), it is possible to retrieve the planet position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ôà Modulation spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, axs = plt.subplots(3, 1, figsize=(15, 10), sharex=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for k in range(3):\n",
    "\n",
    "    print(f\"Kernel {k+1}\")\n",
    "\n",
    "    for key, value in {'Data': data, 'Reference': data_ref}.items():\n",
    "\n",
    "        signal = value[:, k]  # On prend le premier signal pour l'analyse\n",
    "        dt = ctx.interferometer.camera.e.to(u.s).value\n",
    "        N = len(signal)\n",
    "\n",
    "        # Application d'une fen√™tre de Hanning pour r√©duire les effets de bord\n",
    "        window = np.hanning(N)\n",
    "        signal = signal * window\n",
    "\n",
    "        # Zero-padding √†, disons, 10√ó la longueur originale\n",
    "        Npad = 10 * N\n",
    "        padded_signal = np.zeros(Npad)\n",
    "        padded_signal[:N] = signal\n",
    "\n",
    "        # Calcul de la transform√©e de Fourier\n",
    "        frequencies = np.fft.fftfreq(Npad, d=dt)  # axe des fr√©quences\n",
    "        spectrum = np.fft.fft(padded_signal)  # spectre complexe\n",
    "\n",
    "        # On garde uniquement la moiti√© positive (utile pour signaux r√©els)\n",
    "        positive_freqs = frequencies[:Npad // 2]\n",
    "        positive_spectrum = np.abs(spectrum[:Npad // 2])  # module\n",
    "\n",
    "        # Position des maximums locaux\n",
    "        max_indices = np.where(\n",
    "            (positive_spectrum[1:-1] > positive_spectrum[:-2]) & \n",
    "            (positive_spectrum[1:-1] > positive_spectrum[2:])\n",
    "        )[0] + 1\n",
    "        peaks = positive_freqs[max_indices[:3]]\n",
    "\n",
    "        if key == \"Data\":\n",
    "            data_peaks = peaks\n",
    "        else:\n",
    "            ref_peaks = peaks\n",
    "        \n",
    "        print(\"  \", key, \"pics:\", peaks, \"Hz\")\n",
    "\n",
    "        axs[k].plot(positive_freqs, positive_spectrum / np.max(positive_spectrum), label=key)\n",
    "\n",
    "    print(\"   Relative difference:\", np.sum(np.abs(1 - (data_peaks / ref_peaks))))\n",
    "\n",
    "    axs[k].set_title(f'Freq. on kernel {k+1}')\n",
    "    axs[k].legend()\n",
    "    axs[k].set_xlabel('Fr√©quence (Hz)')\n",
    "    axs[k].set_ylabel('Amplitude')\n",
    "    axs[k].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåå On-sky contribution\n",
    "\n",
    "From the obtained data, it is possible to build a 2D distribution of the perceived sky contribution zones. This distribution provides insights into the possible locations of objects, enabling accurate initial estimations to fit the data points obtained based on the parallactic angle.\n",
    "\n",
    "This method involves stacking the transmission maps rotated by the baseline rotation and weighting each map by the corresponding data obtained for that baseline rotation.\n",
    "\n",
    "The base idea was already explored as \"image reconstruction\" technic using classical nulling interferometry $^1$. However, the method here is based on Kernel-Nulls which makes it more complex but less sensitive to phase aberations and by considerig the different Kernels, we can reduce the degeneracy of the solutions.\n",
    "\n",
    "Considering:\n",
    "- $T_{n}$ represents the n-th kernel's normalized transmission map.\n",
    "- $d_{n,\\beta}$ denotes the data point obtained for kernel $n$ with baseline rotation $\\beta$.\n",
    "- $\\theta$ is the parallactic angle.\n",
    "- $\\rho$ is the angular separation.\n",
    "\n",
    "$$\n",
    "r_n(\\rho, \\theta) = \\sum_a T_{n,h}(\\rho,\\theta) d_{n,h}\n",
    "$$\n",
    "\n",
    "\n",
    "As the kernel outputs are antisymetric, we can filter the negative contributions:\n",
    "$$\n",
    "r'_n = \\frac{1}{2}\\max(r_n, 0)\n",
    "$$\n",
    "\n",
    "\n",
    "Finally, we can compute the product over all the kernels to get the final contribution zones:\n",
    "$$\n",
    "C(\\rho, \\theta) = \\prod_n r'_n(\\rho, \\theta)\n",
    "$$\n",
    "\n",
    "**References:**\n",
    "1. Angel, J. R. P., et N. J. Woolf. \"An Imaging Nulling Interferometer to Study Extrasolar Planets\". *The Astrophysical Journal* 475, no 1 (1997): 373‚Äë79. https://doi.org/10.1086/303529."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import analysis\n",
    "from phise import Context\n",
    "from phise import Companion\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "\n",
    "ctx = Context.get_VLTI()\n",
    "ctx.Œì = 1 * u.nm\n",
    "ctx.Œîh = 24 * u.hourangle\n",
    "ctx.interferometer.Œª = 1.65/2 * u.um\n",
    "ctx.interferometer.kn.œÜ = np.zeros(14) * u.nm\n",
    "ctx.interferometer.kn.œÉ = np.zeros(14) * u.nm\n",
    "ctx.interferometer.camera.e = 5 * u.min\n",
    "ctx.target.companions[0].c = 1e-3\n",
    "\n",
    "nb_photons = int(np.sum(ctx.pf).to(1/u.s) * ctx.interferometer.camera.e.to(u.s) * ctx.target.companions[0].c)\n",
    "print(f\"Expected photons from the planet: {nb_photons:.2e}\")\n",
    "\n",
    "analysis.sky_contribution.plot(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ôä Correlation map (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def correlation_map(\n",
    "#         data=HOUR_DIVERSITY_DATA,\n",
    "#         h_range=H_RANGE,\n",
    "#     ):\n",
    "\n",
    "#     resolution = 20\n",
    "    \n",
    "#     _, _, alpha_map, theta_map = get_uv_map(resolution=resolution)\n",
    "\n",
    "#     correl_map = np.zeros((resolution, resolution))\n",
    "\n",
    "#     for x in range(resolution):\n",
    "#         for y in range(resolution):\n",
    "\n",
    "#             alpha = alpha_map[x, y]\n",
    "#             theta = theta_map[x, y]\n",
    "\n",
    "#             km = kernels_modulation(h_range, alpha=alpha, theta=theta)[0]\n",
    "\n",
    "#             correl_map[x, y] = np.sum(np.corrcoef(data[0], km))\n",
    "\n",
    "\n",
    "#     _, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "#     im = ax.imshow(correl_map)\n",
    "#     plt.colorbar(im, ax=ax)\n",
    "#     ax.set_title(\"Correlation map\")\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# correlation_map()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
